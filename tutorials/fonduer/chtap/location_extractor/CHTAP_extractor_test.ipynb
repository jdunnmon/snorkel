{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Create a new database in PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "#set this user line \n",
    "user = 'jared'\n",
    "\n",
    "PARALLEL = 32 # assuming a quad-core machine\n",
    "ATTRIBUTE = \"entity_phone\"\n",
    "os.environ['SNORKELDBNAME'] = \"location_extraction\"\n",
    "\n",
    "if user == 'accenture':\n",
    "    os.environ['SNORKELDB'] = 'postgresql://localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/')\n",
    "elif user == 'jared':\n",
    "    os.environ['SNORKELDB'] = 'postgres://jdunnmon:123@localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/chtap/')\n",
    "elif user == 'jared_local':\n",
    "    os.environ['SNORKELDB'] = 'postgres://jdunnmon:genpass2014@localhost:5432/' + os.environ['SNORKELDBNAME']\n",
    "    sys.path.append(os.environ['SNORKELHOME'] + '/tutorials/fonduer/chtap/')\n",
    "    \n",
    "#from sqlalchemy import create_engine\n",
    "#snorkeldb = create_engine('postgresql://localhost:5432/', isolation_level=\"AUTOCOMMIT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1.1 Defining a Candidate Schema2) Candidate Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import SnorkelSession\n",
    "\n",
    "session = SnorkelSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snorkel.contrib.fonduer.models import candidate_subclass\n",
    "\n",
    "Location_Extraction = candidate_subclass('location_extraction', [\"location\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Parsing and Transforming the Input Documents into Unified Data Models\n",
    "\n",
    "### Configuring an `HTMLPreprocessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer import HTMLPreprocessor, OmniParser\n",
    "\n",
    "if user == 'accenture':\n",
    "    docs_path = os.environ['SNORKELHOME'] + '/tutorials/fonduer/memex/data/profiles_chtap/'\n",
    "elif user == 'jared':\n",
    "    docs_path = '/lfs/local/0/jdunnmon/chtap/data/profiles/crawl_october_2017/texas_profiles_data'\n",
    "elif user == 'jared_local':\n",
    "    docs_path = '/home/jdunnmon/research/re/projects/memex/data/profiles/crawl_october_2017/texas_profiles_data'\n",
    "\n",
    "doc_preprocessor = HTMLPreprocessor(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring an `OmniParser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 15.9 s, sys: 804 ms, total: 16.7 s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "corpus_parser = OmniParser(structural=True, lingual=True)\n",
    "%time corpus_parser.apply(doc_preprocessor, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents: 342\n",
      "Phrases: 71150\n",
      "Table 832\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.models import Document, Phrase,Table\n",
    "\n",
    "print \"Documents:\", session.query(Document).count()\n",
    "print \"Phrases:\", session.query(Phrase).count()\n",
    "print \"Table\", session.query(Table).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Dividing the Corpus into Test and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 274\n",
      "dev: 34\n",
      "test: 34\n"
     ]
    }
   ],
   "source": [
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "ld   = len(docs)\n",
    "\n",
    "train_docs = set()\n",
    "dev_docs   = set()\n",
    "test_docs  = set()\n",
    "splits = (0.8, 0.9)\n",
    "data = [(doc.name, doc) for doc in docs]\n",
    "data.sort(key=lambda x: x[0])\n",
    "for i, (doc_name, doc) in enumerate(data):\n",
    "    if i < splits[0] * ld:\n",
    "        train_docs.add(doc)\n",
    "    elif i < splits[1] * ld:\n",
    "        dev_docs.add(doc)\n",
    "    else:\n",
    "        test_docs.add(doc)\n",
    "from pprint import pprint\n",
    "#pprint([x.name for x in train_docs])\n",
    "print \"train:\",len(train_docs)\n",
    "print \"dev:\" ,len(dev_docs)\n",
    "print \"test:\",len(test_docs)\n",
    "# from pprint import pprint\n",
    "# pprint([x.name for x in train_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Candidate Extraction & Multimodal Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.matchers import *\n",
    "location_matcher = LocationMatcher(longest_match_only=True) \n",
    "\n",
    "####Define a relation's ContextSpaces\n",
    "\n",
    "from snorkel.contrib.fonduer.fonduer.candidates import OmniNgrams\n",
    "location_ngrams = OmniNgrams(n_max=6, split_tokens=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining candidate Throttlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.lf_helpers import *\n",
    "import re\n",
    "from snorkel.lf_helpers import *\n",
    "\n",
    "\n",
    "    \n",
    "def location_currencies_filter(location):\n",
    "    list_currencies = [ \"dollar\", \"dollars\", \"lira\",\"kwacha\",\"rials\",\"rial\",\"dong\",\"dongs\",\"fuerte\",\"euro\",\n",
    "                       \"euros\",\"vatu\",\"som\",\"peso\",\"sterling\",\"sterlings\",\"soms\",\"pestos\",\n",
    "                       \"pounds\", \n",
    "                  \"pound\",\"dirham\",\"dirhams\",\"hryvnia\",\"manat\",\"manats\",\"liras\",\"lira\",\n",
    "                       \"dinar\",\"dinars\",\"pa'anga\",\"franc\",\"baht\",\"schilling\",\n",
    "                  \"somoni\",\"krona\",\"lilangeni\",\"rupee\",\"rand\",\"shilling\",\"leone\",\"riyal\",\"dobra\",\n",
    "                  \"tala\",\"ruble\",\"zloty\",\"peso\",\"sol\",\"quarani\",\"kina\",\"guinean\",\"balboa\",\"krone\",\"naira\",\n",
    "                  \"cordoba\",\"kyat\",\"metical\",\"togrog\",\"leu\",\"ouguiya\",\"rufiyaa\",\"ringgit\",\"kwacha\",\n",
    "                  \"ariary\",\"denar\",\"litas\",\"loti\",\"lats\",\"kip\",\"som\",\"won\",\"tenge\",\"yen\",\"shekel\",\"rupiah\",\n",
    "                  \"forint\",\"lempira\",\"gourde\",\"quetzal\",\"cedi\",\"lari\",\"dalasi\",\"cfp\",\"birr\",\"kroon\",\"nakfa\",\n",
    "                  \"cfa\",\"Peso\",\"koruna\",\"croatian\",\"colon\",\"yuan\",\"escudo\",\"cape\",\"riel\",\"lev\",\"real\"\n",
    "                  ,\"real\",\"mark\",\"boliviano\",\"ngultrum\",\"taka\",\"manat\",\"dram\",\"kwanza\",\"lek\",\"afghani\",\"renminbi\"]\n",
    "\n",
    "    \n",
    "    cand_right_tokens = list(get_right_ngrams(location,window=2))\n",
    "    for cand in cand_right_tokens:\n",
    "        if cand not in list_currencies:\n",
    "            return location\n",
    "    \n",
    "candidate_filter = location_currencies_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "CPU times: user 136 ms, sys: 656 ms, total: 792 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "from snorkel.contrib.fonduer.candidates import CandidateExtractor\n",
    "\n",
    "candidate_extractor = CandidateExtractor(Location_Extraction,\n",
    "                                         [location_ngrams], [location_matcher],\n",
    "                                         candidate_filter=candidate_filter)\n",
    "\n",
    "%time candidate_extractor.apply(train_docs, split=0, parallelism=PARALLEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1534\n"
     ]
    }
   ],
   "source": [
    "train_cands = session.query(Location_Extraction).filter(Location_Extraction.split == 0).all()\n",
    "print \"Number of candidates:\", len(train_cands) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the candidate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.contrib.fonduer.fonduer.lf_helpers import*\n",
    "from snorkel.contrib.fonduer.candidates import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_extraction(Span(\"Corps Christi\", sentence=903876, chars=[39,51], words=[8,9]))\n",
      "location_extraction(Span(\"Houston\", sentence=1007301, chars=[13,19], words=[3,3]))\n",
      "location_extraction(Span(\"Texas\", sentence=904727, chars=[16,20], words=[3,3]))\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print cand_16\n",
    "cand_18= train_cands[18]\n",
    "print cand_18\n",
    "cand_19= train_cands[19]\n",
    "print cand_19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text for the 16th candidate:\n",
      "Phrase (Doc: 508e111f-9936-4e4b-bd25-646724b98018, Index: 0, Text: 361-317-3003 A+ SPA  (Great Massage in Corps Christi) Corpus Christi, Texas Female Escorts)\n",
      "16th candidate\n",
      ": location_extraction(Span(\"Corps Christi\", sentence=903876, chars=[39,51], words=[8,9]))\n",
      "ancestor of 16th candidate\n",
      ": ['html', 'head', 'title']\n",
      "***************************************************\n",
      "text for the 17th candidate:\n",
      "Phrase (Doc: 4d61e08a-5a31-4388-a7a6-68eadd9526c0, Index: 61, Text: var as_sid = '16';     var ad_loc='Dallas';   var aspublisher_width = \"200\";  var aspublisher_height = \"700\";  var aspublis_color_bg = \"ffffff\";  var aspublis_color_border = \"ffffff\";  var aspublis_color_link = \"006621\";  var aspublis_color_text = \"000000\";  var aspublis_color_url = \"1a0dab\";  as_show_ad('page_ads_2', as_sid);)\n",
      "17th candidate: location_extraction(Span(\"Dallas\", sentence=947413, chars=[35,40], words=[11,11]))\n",
      "ancestor of 17th candidate\n",
      ": ['html', 'body', 'div', 'div', 'script']\n",
      "***************************************************\n",
      "text for the 19th candidate:\n",
      "Phrase (Doc: 508e111f-9936-4e4b-bd25-646724b98018, Index: 26, Text: Corpus Christi, Texas Female Escort .)\n",
      "19th candidate: location_extraction(Span(\"Texas\", sentence=904727, chars=[16,20], words=[3,3]))\n",
      "ancestor of 19th candidate\n",
      ": ['html', 'body', 'div', 'div', 'table', 'tr', 'td', 'a']\n"
     ]
    }
   ],
   "source": [
    "cand_16= train_cands[16]\n",
    "print \"text for the 16th candidate:\\n\", cand_16.get_parent()\n",
    "print \"16th candidate\\n:\",cand_16\n",
    "ance_16 = get_ancestor_tag_names(cand_16)\n",
    "print \"ancestor of 16th candidate\\n:\", ance_16 \n",
    "print \"***************************************************\"\n",
    "cand_17= train_cands[17]\n",
    "print \"text for the 17th candidate:\\n\", cand_17.get_parent()\n",
    "print \"17th candidate:\",cand_17\n",
    "ance_17 = get_ancestor_tag_names(cand_17)\n",
    "print \"ancestor of 17th candidate\\n:\", ance_17\n",
    "print \"***************************************************\"\n",
    "\n",
    "cand_19= train_cands[19]\n",
    "print \"text for the 19th candidate:\\n\", cand_19.get_parent()\n",
    "print \"19th candidate:\",cand_19\n",
    "ance_19 = get_ancestor_tag_names(cand_18)\n",
    "print \"ancestor of 19th candidate\\n:\", ance_19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Repeating for development and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 186\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "[========================================] 100%\n",
      "Number of candidates: 222\n",
      "CPU times: user 12.8 s, sys: 1.09 s, total: 13.9 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, docs in enumerate([dev_docs, test_docs]):\n",
    "    candidate_extractor.apply(docs, split=i+1)\n",
    "    print \"Number of candidates:\", session.query(Location_Extraction).filter(Location_Extraction.split == i+1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 1534\n",
      "['html', 'head', 'title']\n",
      "Phrase (Doc: 4d61e08a-5a31-4388-a7a6-68eadd9526c0, Index: 0, Text: 512-796-4441 Ms Austin Dallas, Texas Female Escorts)\n"
     ]
    }
   ],
   "source": [
    "print \"Number of candidates:\", len(train_cands)\n",
    "dev_cand1= dev_cands[10]\n",
    "print get_ancestor_tag_names(dev_cand1)\n",
    "print dev_cand1.get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span(\"Dallas\", sentence=984568, chars=[35,40], words=[11,11])\n",
      "Span(\"Honey Dallas\", sentence=981655, chars=[13,24], words=[1,2])\n",
      "Span(\"United States\", sentence=983667, chars=[0,12], words=[0,1])\n",
      "Span(\"Dallas\", sentence=1005727, chars=[13,18], words=[3,3])\n",
      "Span(\"Dallas\", sentence=983832, chars=[0,5], words=[0,0])\n",
      "Phrase (Doc: 5f0626fa-48e8-4b6b-a7f2-d61bedcbd8f3, Index: 61, Text: var as_sid = '16';     var ad_loc='Dallas';   var aspublisher_width = \"200\";  var aspublisher_height = \"700\";  var aspublis_color_bg = \"ffffff\";  var aspublis_color_border = \"ffffff\";  var aspublis_color_link = \"006621\";  var aspublis_color_text = \"000000\";  var aspublis_color_url = \"1a0dab\";  as_show_ad('page_ads_2', as_sid);)\n",
      "Phrase (Doc: 5f0626fa-48e8-4b6b-a7f2-d61bedcbd8f3, Index: 0, Text: 469-835-3804 Honey Dallas, Texas Female Escorts)\n",
      "Phrase (Doc: 5f0626fa-48e8-4b6b-a7f2-d61bedcbd8f3, Index: 20, Text: United States »)\n",
      "Phrase (Doc: 5f0626fa-48e8-4b6b-a7f2-d61bedcbd8f3, Table: 0, Row: 0, Col: 0, Index: 0, Text: Back To All  Dallas, Texas Female Escort)\n",
      "Phrase (Doc: 5f0626fa-48e8-4b6b-a7f2-d61bedcbd8f3, Index: 28, Text: Dallas, Texas Female Escort available for Incall.)\n"
     ]
    }
   ],
   "source": [
    "can_tmp = []\n",
    "dc_ind=50\n",
    "for can in train_cands:\n",
    "    if can.get_parent().document.name == dev_cands[dc_ind].get_parent().document.name:\n",
    "        can_tmp.append(can.get_parent())\n",
    "        print can.location\n",
    "for cn in can_tmp:\n",
    "    print cn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Place Names and Locationsfrom Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting google place and geocoding APIs\n",
    "import googlemaps as gm\n",
    "import gmaps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "maps_api_key = 'AIzaSyA0Veo5Lc6JOwDjNgQvPEhQB4AiZcrYQGI'\n",
    "gmaps.configure(api_key=maps_api_key)\n",
    "\n",
    "def get_possible_locations(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo: full json structure returned from API call\n",
    "    cl: list of candidate location strings\n",
    "    \"\"\" \n",
    "    api_key = 'AIzaSyDbk3lLZHuQVKDRBN99_oz-p4AJjIzhA0w'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.places.places_autocomplete(gms,plc)\n",
    "    cl = [a['description'] for a in qo]\n",
    "    return qo,cl\n",
    "\n",
    "def get_geocode(plc):\n",
    "    \"\"\"\n",
    "    INPUTS\n",
    "    plc: string describing place to match\n",
    "\n",
    "    OUTPUTS\n",
    "    qo full json structure returned from API call\n",
    "    (lat,lon): lat-lon tuple\n",
    "    \"\"\"\n",
    "    api_key = 'AIzaSyBlLyOaasYMgMxFGUh2jJyxIG0_pZFF_jM'\n",
    "    gms = gm.Client(key=api_key)\n",
    "    qo = gm.geocoding.geocode(gms,plc)\n",
    "    lat = qo[0]['geometry']['location']['lat']\n",
    "    lng = qo[0]['geometry']['location']['lng']\n",
    "    return qo,(lat,lng)\n",
    "\n",
    "def slice_pd_by_cont(dfm,col,val,pres=True,lower=False,union=False):\n",
    "    \"\"\"\n",
    "    Returns dataframe where column values include/exclude values in provided list\n",
    "    \n",
    "    INPUTS:\n",
    "    dfm: dataframe\n",
    "    col: column header\n",
    "    val: list of strings to include/ignore\n",
    "    pres: true to include, false to exclude\n",
    "    union: include union of these values\n",
    "    \"\"\"\n",
    "    if union:\n",
    "        val = ['|'.join(val)]\n",
    "    for vl in val:\n",
    "        if ~lower:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.contains(vl,na=False)]\n",
    "        else:\n",
    "            if pres:\n",
    "                dfm = dfm.loc[dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "            else:\n",
    "                dfm = dfm.loc[~dfm[col].str.lower().str.contains(vl,na=False)]\n",
    "    return dfm\n",
    "\n",
    "def map_candidates_and_centroid(dfm):\n",
    "    \"\"\"\n",
    "    INPUT\n",
    "    dfm: dataframe containing at least latitude, longitude\n",
    "    \n",
    "    OUTPUT\n",
    "    centroid: np array of lat/lon of location centroid\n",
    "    \"\"\"\n",
    "    df_cans = dfm\n",
    "    df_cans_map = dfm[['latitude','longitude']]\n",
    "    df_cans['lat_long'] = df_cans[['latitude', 'longitude']].apply(tuple, axis=1)\n",
    "    point_tup_lst = df_cans['lat_long'].tolist()\n",
    "    points = MultiPoint(point_tup_lst)\n",
    "    cent = np.array(points.centroid)\n",
    "    cent_df = pd.DataFrame([cent]) #this is a rough centroid estimate\n",
    "    fig = gmaps.Map()\n",
    "    can_layer = gmaps.symbol_layer(\n",
    "    df_cans_map, fill_color=\"green\", stroke_color=\"green\", scale=2)\n",
    "    cent_layer = gmaps.symbol_layer(\n",
    "    cent_df, fill_color=\"red\", stroke_color=\"red\", scale=2)\n",
    "    fig.add_layer(can_layer)\n",
    "    fig.add_layer(cent_layer)\n",
    "    fig\n",
    "    return cent,fig\n",
    "\n",
    "state_dict = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}\n",
    "state_add_dict = {v: k for k, v in state_dict.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a candidate dictionary keyed by document name\n",
    "doc_dict = defaultdict(list)\n",
    "loc_dict = defaultdict(list)\n",
    "can_loc_dict = defaultdict(list)\n",
    "for can in dev_cands:\n",
    "    doc_dict[can.get_parent().document.name].append(can)\n",
    "\n",
    "    #calling API for each location\n",
    "for ky in doc_dict.keys():\n",
    "    #print doc_dict[ky]\n",
    "    for can in doc_dict[ky]:\n",
    "        loc_can = can.location.get_span()\n",
    "        can_loc_dict[ky].append(loc_can)\n",
    "    #for plc in list(set(can_loc_dict[ky])):   \n",
    "        #_,loc_out = get_possible_locations(plc)\n",
    "        #loc_dict[ky] = loc_dict[ky]+loc_out\n",
    "  #  print loc_dict[ky] \n",
    "  #  print can_loc_dict[ky]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "from collections import Counter\n",
    "    \n",
    "def get_attr(obj):\n",
    "    out = [a for a in dir(obj) if not a.startswith('__') and not callable(getattr(obj,a))]\n",
    "    return out\n",
    "\n",
    "def most_common(lt):\n",
    "    data = Counter(lt)\n",
    "    return data.most_common(1)[0][0]\n",
    "\n",
    "def get_common_country(lt):\n",
    "    country_lst = []\n",
    "    country_els = []\n",
    "    for it in lt:\n",
    "        try:\n",
    "            country = pycountry.countries.lookup(it.lower())\n",
    "            country_lst.append(country.alpha_3)\n",
    "            country_els.append(it)\n",
    "        except:\n",
    "            country = None \n",
    "    if country_lst == []:\n",
    "        return 'none',[],[]\n",
    "    return most_common(country_lst),country_lst, country_els\n",
    "\n",
    "def get_common_state(lt):\n",
    "    state_lst = []\n",
    "    state_els = []\n",
    "    for it in lt:\n",
    "        if it in state_add_dict.keys():\n",
    "            state_lst.append(it)\n",
    "            state_els.append(it)\n",
    "        elif it in state_add_dict.values():\n",
    "            state_lst.append(state_dict[it])\n",
    "            state_els.append(it)\n",
    "    if state_lst == []:\n",
    "        return 'none',[],[]\n",
    "    else:\n",
    "        return most_common(state_lst), state_lst, state_els\n",
    "\n",
    "def get_possible_locale(lt,cn,st,cn_lst,st_lst):\n",
    "    locale_list = []\n",
    "    a = [b for b in lt if b not in cn_lst and b not in st_lst]\n",
    "    for b in a:\n",
    "        locales = get_possible_locations(b)\n",
    "        locales = [c for c in locales if cn in b and st in b]\n",
    "        locale_list.append(locales)\n",
    "    return locale_list\n",
    "\n",
    "def lookup_country_name(cn):\n",
    "    try:\n",
    "        out = pycountry.countries.lookup(cn).name\n",
    "    except:\n",
    "        out = 'no country'\n",
    "    return out\n",
    "\n",
    "def lookup_state_abbrev(cn):\n",
    "    try:\n",
    "        out = state_add_dict[cn]\n",
    "    except:\n",
    "        out = 'no state'\n",
    "    return out\n",
    "\n",
    "def lookup_country_alpha3(cn):\n",
    "    return pycountry.countries.lookup(cn).alpha_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[u'none', u'none', u'TX', u'USA']\n",
      "[u'San Antonio', u'San Antonio', u'San Antonio', u'San Antonio', u'Southsidekat San Antonio']\n",
      "2\n",
      "> <ipython-input-80-46674d11d0bc>(38)<module>()\n",
      "-> not_exact = 1\n",
      "(Pdb) aset\n",
      "[u'Southsidekat San Antonio', u'San Antonio']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-46674d11d0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mnot_exact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mnot_exact\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-46674d11d0bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mnot_exact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mnot_exact\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocales\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/bdb.pyc\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36muser_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_mainpyfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbp_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/pdb.pyc\u001b[0m in \u001b[0;36minteraction\u001b[0;34m(self, frame, traceback)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_stack_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmdloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/cmd.pyc\u001b[0m in \u001b[0;36mcmdloop\u001b[0;34m(self, intro)\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_rawinput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'EOF'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lfs/local/0/jdunnmon/repos/anaconda3/envs/py27snorkel/lib/python2.7/site-packages/ipykernel/kernelbase.pyc\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#getting most common locale\n",
    "out_locales = defaultdict(list)\n",
    "out_countries = defaultdict(list)\n",
    "out_states = defaultdict(list)\n",
    "for idx,ky in enumerate(can_loc_dict.keys()):\n",
    "    #getting country names\n",
    "    probable_country,country_list, country_els = get_common_country(can_loc_dict[ky])\n",
    "    out_countries[ky] = probable_country\n",
    "    #if pycountry.countries.lookup(probable_country).alpha_3 == 'USA' and len(can_loc_dict[ky]) >1:\n",
    "        #getting state names\n",
    "    probable_state,state_list,state_els = get_common_state(can_loc_dict[ky])\n",
    "   # else:\n",
    "   #     probable_state,state_list,state_els = 'none',[],[]\n",
    "    out_states[ky] = probable_state\n",
    "    \n",
    "    #getting state names\n",
    "    locale_list = []\n",
    "    a = [b for b in can_loc_dict[ky] if b not in country_els and b not in state_els] #need lookup here\n",
    "    print a\n",
    "    if a == []:\n",
    "        if probable_state != 'none' and probable_country != 'none':\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "        elif probable_state != 'none' and probable_country == 'none':\n",
    "            probable_country = 'USA'\n",
    "            locale_list = ['none,none,'+state_add_dict[probable_state]+','+probable_country]\n",
    "        elif probable_state == 'none' and probable_country != 'none':\n",
    "            locale_list = ['none,none,'+'none'+','+probable_country]\n",
    "    else:\n",
    "        most_common_locale = most_common(a)\n",
    "        aset = list(set(a))\n",
    "        print len(aset)\n",
    "        for b in aset:\n",
    "                locale_tmp = []\n",
    "                try:\n",
    "                    qo,locales = get_possible_locations(b)\n",
    "                except:\n",
    "                    import pdb; pdb.set_trace()\n",
    "                not_exact = 1\n",
    "                count = 0\n",
    "                while not_exact and count<len(locales):\n",
    "                    print('Checking Locale %d of %d' %(count,len(locales)))\n",
    "                    c = locales[count]\n",
    "                    spl =  [str(x.strip().lower()) for x in c.split(',')]\n",
    "                    if lookup_country_name(probable_country).lower() in spl:\n",
    "                        if lookup_state_abbrev(probable_state).lower() in spl: \n",
    "                            if spl[0].lower() == most_common_locale.lower() and len(spl) == 3:\n",
    "                                locale_list = ['none']+spl\n",
    "                                locale_list = [','.join(locale_list)]\n",
    "                                not_exact = 0\n",
    "                                print 'Exact City Found'\n",
    "                            elif spl[0].lower() == most_common_locale.lower() and len(spl) == 4:\n",
    "                                locale_list = [','.join(spl)]\n",
    "                                not_exact = 0\n",
    "                                print 'Exact Location Found'\n",
    "                            else:             \n",
    "                                locale_list.append(','.join(spl))  \n",
    "                                count = count+1\n",
    "                        else:\n",
    "                            if out_states[ky] == 'none':\n",
    "                                locale_list.append(','.join(spl))\n",
    "                            count = count+1         \n",
    "                    else:\n",
    "                        count = count+1\n",
    "        \n",
    "    #reformatting for labeling comparison\n",
    "    locale_list_out = []\n",
    "    for c in locale_list:\n",
    "        b = c.split(',')\n",
    "        state_lst = [s.lower() for s in state_dict.values()]\n",
    "        print b\n",
    "        if b[-1] != 'none': b[-1] = str(lookup_country_alpha3(b[-1]).lower()) \n",
    "        if b[-2] != 'none': \n",
    "            if b[-2].lower() in state_lst:\n",
    "                b[-2] = b[-2].lower() \n",
    "            else:\n",
    "                b[-2] = state_dict[b[-2].upper()].lower() \n",
    "        locale_list_out.append(','.join(b)) \n",
    "    out_locales[ky] = locale_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in out_locales.keys():\n",
    "    if out_locales[ii] == []:\n",
    "        out_locales[ii] = ['none','none','none','none']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('extracted_loc_tsv_all_emissions.tsv','w')\n",
    "f.write('Document'+\"\\t\"+\"CoreNLP Matcher Output\"+\"\\t\"+\"Most Likely Country\"+\"\\t\"+\"Most Likely State\"+\"Extractor Emission\"+\"\\n\")\n",
    "for ky in can_loc_dict.keys():\n",
    "    line = ky+\"\\t\"+';'.join(can_loc_dict[ky])+\"\\t\"+out_countries[ky]+\"\\t\" + out_states[ky]+\"\\t\"+';'.join(out_locales[ky])+'\\n'\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print len(can_loc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
